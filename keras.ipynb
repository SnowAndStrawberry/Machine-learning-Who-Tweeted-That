{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP90051 Project 1 source code\n",
    "# For Team 192\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# self-made external scripts\n",
    "import ext_scipy\n",
    "import ext_preprocess\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(col_name):\n",
    "    df_X_train = pd.read_csv('cleaned_train.csv')\n",
    "    df_X_test = pd.read_csv('cleaned_test.csv')\n",
    "    \n",
    "    all_labels = np.sort(df_X_train['label'].unique())\n",
    "\n",
    "    #df_X_train = df_X_train[df_X_train[col_name].notna()]\n",
    "    df_X_train[col_name] = df_X_train[col_name].fillna('')\n",
    "    df_X_train, df_X_validation = train_test_split(df_X_train,\n",
    "                                                   test_size=config['VALIDATION_SIZE'],\n",
    "                                                   random_state=config['RAND_STATE_NUM'])\n",
    "\n",
    "    X_train, y_train = df_X_train[col_name], df_X_train['label']\n",
    "    X_validation, y_validation = df_X_validation[col_name], df_X_validation['label']\n",
    "\n",
    "    na_spots = df_X_test[col_name].isna()\n",
    "    df_X_test[col_name][na_spots] = df_X_test['tweet'][na_spots]\n",
    "    X_test = df_X_test[col_name]\n",
    "    \n",
    "    return X_train, y_train, X_validation, y_validation, X_test, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'RAND_STATE_NUM': 5354,\n",
    "    'VERSION': 0,\n",
    "    'VALIDATION_SIZE': 0.0345\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, all_labels = load_dataset('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "N_GRAM_LEN = 3\n",
    "cv = CountVectorizer(ngram_range=(N_GRAM_LEN, N_GRAM_LEN), lowercase=False, analyzer='char_wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246688\n"
     ]
    }
   ],
   "source": [
    "vocab_size_ngram = len(cv.vocabulary_) + 1\n",
    "print(vocab_size_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with train_convolve\n"
     ]
    }
   ],
   "source": [
    "def map_ints(tweet):\n",
    "    return np.array([k for k in map(cv.vocabulary_.get, cv.build_analyzer()(tweet)) if k is not None]) + 1\n",
    "\n",
    "X_train_ngram = X_train.apply(map_ints)\n",
    "print(\"Done with train_convolve\")\n",
    "X_validation_ngram = X_validation.apply(map_ints)\n",
    "X_test_ngram = X_test.apply(map_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def pad_set(train, vald, test):\n",
    "    maxlen = max([len(x) for x in train]) + 5\n",
    "    return pad_sequences(train, padding='post', maxlen=maxlen), pad_sequences(vald, padding='post', maxlen=maxlen), pad_sequences(test, padding='post', maxlen=maxlen), maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ngram, X_validation_ngram, X_test_ngram, maxlen_ngram = pad_set(X_train_ngram, X_validation_ngram, X_test_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(all_labels)\n",
    "\n",
    "y_train_enc = np_utils.to_categorical(encoder.transform(y_train), num_classes=encoder.classes_.shape[0])\n",
    "y_validation_enc = np_utils.to_categorical(encoder.transform(y_validation), num_classes=encoder.classes_.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Prepare embedding for cleaned_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, _ = load_dataset('cleaned_v3')\n",
    "\n",
    "tokenizer = Tokenizer() # num_words=config['TOKEN_WORDS'])\n",
    "tokenizer.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = tokenizer.texts_to_sequences(X_train)\n",
    "X_validation_all = tokenizer.texts_to_sequences(X_validation)\n",
    "X_test_all = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size_all = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, X_validation_all, X_test_all, maxlen_all = pad_set(X_train_all, X_validation_all, X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((317583, 42), 42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ngram.shape, maxlen_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((316681, 72), 72)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all.shape, maxlen_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "input1 = layers.Input(shape=(maxlen_ngram,))\n",
    "em_layer = layers.Embedding(vocab_size_ngram, embedding_dim)(input1)\n",
    "\n",
    "CNN_SETTINGS = [[384, 3, 3]]\n",
    "\n",
    "for n_filters, kernel_width, pool_size in CNN_SETTINGS:\n",
    "    f = layers.Conv1D(n_filters, kernel_width, activation='relu')(em_layer)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.SpatialDropout1D(0.15)(f)\n",
    "    f = layers.AveragePooling1D(pool_size)(f)\n",
    "   # f = layers.MaxPooling1D(pool_size)(f)\n",
    "\n",
    "#f = layers.AveragePooling1D(pool_size)(f)\n",
    "f = layers.GlobalMaxPooling1D()(f)\n",
    "#f = layers.Dense(128, activation='relu')(f)\n",
    "f = layers.Dense(all_labels.shape[0], activation='softmax')(f)\n",
    "\n",
    "model = Model(inputs=input1, outputs=f)\n",
    "opt = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 120\n",
    "\n",
    "input1 = layers.Input(shape=(maxlen_ngram,))\n",
    "em_layer = layers.Embedding(vocab_size_ngram, embedding_dim)(input1)\n",
    "\n",
    "f1 = layers.Conv1D(192, 2, activation='relu')(em_layer)\n",
    "f1 = layers.MaxPooling1D()(f1)\n",
    "f1 = layers.GlobalMaxPooling1D()(f1)\n",
    "\n",
    "f2 = layers.Conv1D(192, 3, activation='relu')(em_layer)\n",
    "f2 = layers.MaxPooling1D()(f2)\n",
    "f2 = layers.GlobalMaxPooling1D()(f2)\n",
    "\n",
    "f3 = layers.Conv1D(192, 4, activation='relu')(em_layer)\n",
    "f3 = layers.MaxPooling1D()(f3)\n",
    "f3 = layers.GlobalMaxPooling1D()(f3)\n",
    "\n",
    "f = layers.Concatenate(axis=1)([f1, f2, f3])\n",
    "#f = layers.Flatten()(mrg)\n",
    "#f = layers.Dense(128, activation='relu')(f)\n",
    "f = layers.Dense(all_labels.shape[0], activation='softmax')(f)\n",
    "\n",
    "model = Model(inputs=input1, outputs=f)\n",
    "opt = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit([X_train_all, X_train_ngram], y_train_enc,\n",
    "                    epochs=4, batch_size=200,\n",
    "                    validation_data=([X_validation_all, X_validation_ngram], y_validation_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_ngram, y_train_enc,\n",
    "                    epochs=4, batch_size=300,\n",
    "                    validation_data=(X_validation_ngram, y_validation_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import json\n",
    "\n",
    "plot_model(model, to_file='./keras_out/model.png')\n",
    "json.dump(history.history, open('./keras_out/history.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_keras_results(name):\n",
    "    model.save(name + \".h5\")\n",
    "    json.dump(history.history, open(name + ' [HISTORY].json', 'w'))\n",
    "    json.dump(config, open(name + ' [CONFIG].json', 'w'))\n",
    "    \n",
    "save_keras_results(config['MODEL_NAME'] + \"_v%d\" % (config['VERSION']))\n",
    "config['VERSION'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() # to start again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(X_train_embed.shape[0], 100000, replace=False)\n",
    "\n",
    "#model.evaluate(x=X_train_embed[sample], y=y_train_enc[sample])\n",
    "model.evaluate(x=X_validation_embed, y=y_validation_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_pre = model.predict_classes(X_test_embed)\n",
    "predictions_pre = np.argmax(model.predict(X_test_ngram), axis=-1)\n",
    "\n",
    "prediction_ = np.argmax(np_utils.to_categorical(predictions_pre), axis = 1)\n",
    "prediction_ = encoder.inverse_transform(prediction_)\n",
    "\n",
    "predictions = prediction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_scipy.save_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
