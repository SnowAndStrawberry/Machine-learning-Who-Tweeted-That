{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP90051 Project 1 source code\n",
    "# For Team 192\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# self-made external scripts\n",
    "import ext_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'ROW_USE': 'tweet',\n",
    "    'RANDOM_STATE': 1569198,\n",
    "    'COMMENT': 'On new data set'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = pd.read_csv('cleaned_train.csv')\n",
    "df_X_test = pd.read_csv('cleaned_test.csv')\n",
    "\n",
    "all_unique = df_X_train['label'].unique()\n",
    "\n",
    "df_X_train = df_X_train[df_X_train[config['ROW_USE']].notna()]\n",
    "X_train, y_train = df_X_train[config['ROW_USE']].to_numpy(), df_X_train['label'].to_numpy()\n",
    "\n",
    "df_X_test[config['ROW_USE']][df_X_test[config['ROW_USE']].isna()] = df_X_test[\"tweet\"][df_X_test[config['ROW_USE']].isna()]\n",
    "X_test = df_X_test[config['ROW_USE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, FunctionTransformer, StandardScaler\n",
    "from sklearn.kernel_approximation import Nystroem, RBFSampler\n",
    "\n",
    "vectorizer_params = {\n",
    "    'strip_accents': 'unicode',\n",
    "    #token_pattern='(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n",
    "    'analyzer': 'word', #'max_df': 3.1e-6,\n",
    "    'lowercase': True, 'stop_words': 'english', 'n_features': 2**15, 'alternate_sign': False, # 'norm': None\n",
    "}\n",
    "\n",
    "vectorizer_params_2 = {\n",
    "    'strip_accents': 'unicode',\n",
    "    #token_pattern='(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n",
    "    'analyzer': 'word', 'max_features': 32768, #\n",
    "    'lowercase': True, 'stop_words': 'english'\n",
    "}\n",
    "\n",
    "lda_params = {\n",
    "    'n_components': 1500,\n",
    "    'learning_method': 'online',\n",
    "    'batch_size': 2000,\n",
    "    'random_state': config['RANDOM_STATE'], 'n_jobs': 1, 'verbose': 2\n",
    "}\n",
    "\n",
    "\n",
    "nystroem_params = {\n",
    "    'random_state': config['RANDOM_STATE'],\n",
    "    'n_components': 2**5,\n",
    "    'gamma': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn.random_projection as rp\n",
    "\n",
    "# Vectorize the text\n",
    "fp_vectorize = Pipeline([\n",
    "    ('vc', CountVectorizer(**vectorizer_params_2)),\n",
    "    #('tfidf', TfidfTransformer())\n",
    "], verbose=True)\n",
    "\n",
    "# put it altogether\n",
    "fp_all = fp_vectorize # make_pipeline(fp_vectorize, fp_dim_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# using library from https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "# sentiment\n",
    "def get_vader_sentiment(X, **args):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return np.array([analyzer.polarity_scores(tw)['compound'] for tw in args['tweets']]).reshape(-1, 1)\n",
    "\n",
    "# length of tweet\n",
    "def get_text_length(X, **args):\n",
    "    return np.array([len(tw) for tw in args['tweets']]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "new_pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('main', Pipeline([\n",
    "            #('vectorizer', HashingVectorizer(**vectorizer_params)),\n",
    "            ('tfidf', TfidfVectorizer(**vectorizer_params_2)),\n",
    "            #('svd', TruncatedSVD(n_components=300, random_state=config['RANDOM_STATE']))\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('lenf', FunctionTransformer(get_text_length, validate=False)),\n",
    "            ('lennorm', StandardScaler()),\n",
    "        ])),\n",
    "        ('sent', Pipeline([\n",
    "            ('sentf', FunctionTransformer(get_vader_sentiment, validate=False)),\n",
    "            ('sentnorm', StandardScaler()),\n",
    "        ]))\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_all = new_pipe\n",
    "\n",
    "X_tweets = df_X_train['tweet'].to_numpy()\n",
    "fp_all.set_params(feats__length__lenf__kw_args={'tweets': X_tweets}, feats__sent__sentf__kw_args={'tweets': X_tweets})\n",
    "X_train_transformed = fp_all.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp_all.fit(np.concatenate((X_train, X_test)))\n",
    "X_train_transformed = fp_all.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweets = df_X_test['tweet'].to_numpy()\n",
    "fp_all.set_params(feats__length__lenf__kw_args={'tweets': X_tweets}, feats__sent__sentf__kw_args={'tweets': X_tweets})\n",
    "X_test_cv = fp_all.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open('np_train_svd.npy', 'wb'), X_train_transformed)\n",
    "np.save(open('np_test_svd.npy', 'wb'), X_test_cv)\n",
    "\n",
    "#X_train_transformed = np.load(open('np_train_svd.npy', 'rb'))\n",
    "#X_test_cv = np.load(open('np_test_svd.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328932, 32768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "accs = []\n",
    "X_train_2, X_vald, y_train_2, y_vald = train_test_split(X_train_transformed, y_train, random_state=config['RANDOM_STATE'], test_size=0.035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perceptron: loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None\n",
    "\n",
    "inc = ext_scipy.IncrementalLearn(SGDClassifier(n_jobs=-1, alpha=1e-6, warm_start=True, loss='hinge', penalty='l2'), batch_amount=30000)\n",
    "#inc = ext_scipy.IncrementalLearn(SGDClassifier(n_jobs=-1, warm_start=True, loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None), batch_amount=30000)\n",
    "#inc = ext_scipy.IncrementalLearn(MultinomialNB(alpha=0.01), batch_amount=8000)\n",
    "#inc = ext_scipy.IncrementalLearn(SGDClassifier(n_jobs=-1, warm_start=True, loss=\"log\", alpha=1e-8), batch_amount=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model_incremental(inc, epochs, X_train_use, y_train_use, X_vald, y_vald, accuracy_list):\n",
    "    for i in range(epochs):\n",
    "        time_start = time.time()\n",
    "        X, y = shuffle(X_train_use, y_train_use)\n",
    "        inc.fit(X, y, extra_text='Progress: ' + str(accuracy_list), classes=all_unique)\n",
    "        \n",
    "        if X_vald is not None and y_vald is not None:\n",
    "            accuracy_list.append(accuracy_score(y_vald, ext_scipy.predict_batch(X_vald, inc.model.predict, batch_amount=8000)))\n",
    "    return inc\n",
    "        \n",
    "def get_accuracy(model_pred_f, X_train_use, y_train_use, prop=0.08, batch_amount=5000):\n",
    "    sample_tr = np.random.choice(X_train_use.shape[0], int(X_train_use.shape[0] * prop))\n",
    "    preds = ext_scipy.predict_batch(X_train_use[sample_tr], model_pred_f, batch_amount=batch_amount)\n",
    "\n",
    "    return accuracy_score(y_train_use[sample_tr], preds)\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "# Perhaps use metrics.classification_report for more detailed report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8000 / 12300 (0.6504, 25.559 s)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vald_preds = ext_scipy.predict_batch(X_vald, inc.model.predict, batch_amount=8000)\n",
    "# df_X_vald.iloc[np.nonzero(y_vald == vald_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8043089430894309"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(vald_preds, y_vald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_vald, vald_preds, average='micro') # of all actual positives, which ones were correct\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_vald, vald_preds, average='micro') # of all predicted positive, which ones were correct\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_vald, vald_preds, average='micro')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_vald, vald_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8000 / 11513 (0.6949, 32.717 s)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inc = train_model_incremental(inc, 5, X_train_2, y_train_2, X_vald, y_vald, accs)\n",
    "\n",
    "#train_model_incremental(inc, 1, X_train_transformed, y_train, None, None, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f_name = 'naive_bayes_results.pkl'\n",
    "\n",
    "res = pickle.load(open(f_name, 'rb'))\n",
    "#res = []\n",
    "res.append({'model_name': type(inc.model).__name__, \\\n",
    "            'model_params': inc.model.get_params(), \\\n",
    "            'pipeline_params': fp_all.get_params()['steps'], \\\n",
    "            'extra_params': inc.params_, \\\n",
    "            'other': config, \\\n",
    "            'accs': accs,\n",
    "            'train_acc': train_acc})\n",
    "\n",
    "#res[-1]['test_val'] = 0.20308\n",
    "\n",
    "pickle.dump(res, open(f_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "vm_res = json.load(open('svm_results.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#joblib.dump(inc, 'svm_final.pkl') \n",
    "inc = joblib.load('svm_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30000 / 35437 (0.8466, 19.978 s)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_final = fp_all.transform(X_test)\n",
    "\n",
    "predictions = ext_scipy.predict_batch(X_test_final, inc.model.predict, \\\n",
    "                                      batch_amount=10000, sparse_expand=False)\n",
    "#predictions = model_use.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_scipy.save_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35437, 80000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
